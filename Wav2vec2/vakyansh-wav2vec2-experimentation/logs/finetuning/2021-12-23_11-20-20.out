[2021-12-23 11:20:24,529][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/logs/finetuning/tensorboard_2021-12-23_11-20-20', 'wandb_project': 'ieee_worksop', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [24], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning', 'restore_file': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/pretraining/CLSRIL-23.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.05, 'normalize': False, 'data': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/data/finetuning', 'w2v_args': None, 'mask_min_space': 1, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768}, 'task': {'_name': 'audio_pretraining', 'data': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/data/finetuning', 'labels': 'ltr', 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'autoregressive': False, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'tpu': False}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 20000, 'lr': [5e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2021-12-23 11:20:27,883][fairseq_cli.train][INFO] - Wav2VecCtc(
  (w2v_encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU()
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU()
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.0, inplace=False)
    (proj): Linear(in_features=768, out_features=8, bias=True)
  )
)
[2021-12-23 11:20:27,886][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2021-12-23 11:20:27,886][fairseq_cli.train][INFO] - model: Wav2VecCtc
[2021-12-23 11:20:27,886][fairseq_cli.train][INFO] - criterion: CtcCriterion
[2021-12-23 11:20:27,887][fairseq_cli.train][INFO] - num. shared model params: 94,377,864 (num. trained: 94,377,864)
[2021-12-23 11:20:27,889][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2021-12-23 11:20:27,891][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 1080, skipped 0 samples
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
[2021-12-23 11:20:31,142][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
[2021-12-23 11:20:31,143][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2021-12-23 11:20:31,143][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.915 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2021-12-23 11:20:31,143][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2021-12-23 11:20:31,143][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2021-12-23 11:20:31,144][fairseq_cli.train][INFO] - max tokens per device = 3200000 and max sentences per device = None
[2021-12-23 11:20:31,145][fairseq.trainer][INFO] - Preparing to load checkpoint /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt
[2021-12-23 11:20:31,146][fairseq.trainer][INFO] - No existing checkpoint found /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt
[2021-12-23 11:20:31,146][fairseq.trainer][INFO] - loading train data for epoch 1
[2021-12-23 11:20:31,155][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 8618, skipped 0 samples
[2021-12-23 11:20:31,291][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster
wandb: Currently logged in as: vaishshells (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.5
wandb: Syncing run finetuning
wandb: â­ï¸ View project at https://wandb.ai/vaishshells/ieee_worksop
wandb: ðŸš€ View run at https://wandb.ai/vaishshells/ieee_worksop/runs/2r4n3www
wandb: Run data is saved locally in /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/scripts/finetuning/outputs/2021-12-23/11-20-24/wandb/run-20211223_112032-2r4n3www
wandb: Run `wandb offline` to turn off syncing.

[2021-12-23 11:20:37,186][fairseq.trainer][INFO] - begin training epoch 1
[2021-12-23 11:20:37,187][fairseq_cli.train][INFO] - Start iterating over samples
[2021-12-23 11:20:38,299][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB (GPU 0; 10.92 GiB total capacity; 3.97 GiB already allocated; 42.50 MiB free; 3.99 GiB reserved in total by PyTorch)
[2021-12-23 11:20:38,300][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4069 MB |    4069 MB |    9117 MB |    5048 MB |
|       from large pool |    4065 MB |    4065 MB |    9113 MB |    5047 MB |
|       from small pool |       3 MB |       3 MB |       3 MB |       0 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4069 MB |    4069 MB |    9117 MB |    5048 MB |
|       from large pool |    4065 MB |    4065 MB |    9113 MB |    5047 MB |
|       from small pool |       3 MB |       3 MB |       3 MB |       0 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4088 MB |    4088 MB |    4088 MB |       0 B  |
|       from large pool |    4084 MB |    4084 MB |    4084 MB |       0 B  |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   19135 KB |  652800 KB |    5055 MB |    5036 MB |
|       from large pool |   18565 KB |  651792 KB |    5051 MB |    5033 MB |
|       from small pool |     570 KB |    2038 KB |       3 MB |       2 MB |
|---------------------------------------------------------------------------|
| Allocations           |     410    |     410    |     610    |     200    |
|       from large pool |     222    |     222    |     331    |     109    |
|       from small pool |     188    |     188    |     279    |      91    |
|---------------------------------------------------------------------------|
| Active allocs         |     410    |     410    |     610    |     200    |
|       from large pool |     222    |     222    |     331    |     109    |
|       from small pool |     188    |     188    |     279    |      91    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      44    |      44    |      44    |       0    |
|       from large pool |      42    |      42    |      42    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      15    |      17    |     159    |     144    |
|       from large pool |      13    |      15    |     114    |     101    |
|       from small pool |       2    |       3    |      45    |      43    |
|===========================================================================|

[2021-12-23 11:20:38,301][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:38,301][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:39,908][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.13 GiB (GPU 0; 10.92 GiB total capacity; 2.75 GiB already allocated; 178.50 MiB free; 4.79 GiB reserved in total by PyTorch)
[2021-12-23 11:20:39,908][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    2815 MB |    5378 MB |   34122 MB |   31307 MB |
|       from large pool |    2811 MB |    5374 MB |   34110 MB |   31299 MB |
|       from small pool |       3 MB |       4 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| Active memory         |    2815 MB |    5378 MB |   34122 MB |   31307 MB |
|       from large pool |    2811 MB |    5374 MB |   34110 MB |   31299 MB |
|       from small pool |       3 MB |       4 MB |      11 MB |       7 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4910 MB |    5562 MB |    8932 MB |    4022 MB |
|       from large pool |    4904 MB |    5556 MB |    8926 MB |    4022 MB |
|       from small pool |       6 MB |       6 MB |       6 MB |       0 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    2094 MB |    3031 MB |   24070 MB |   21976 MB |
|       from large pool |    2092 MB |    3031 MB |   24057 MB |   21965 MB |
|       from small pool |       2 MB |       2 MB |      13 MB |      11 MB |
|---------------------------------------------------------------------------|
| Allocations           |     377    |     426    |    1818    |    1441    |
|       from large pool |     145    |     225    |    1002    |     857    |
|       from small pool |     232    |     238    |     816    |     584    |
|---------------------------------------------------------------------------|
| Active allocs         |     377    |     426    |    1818    |    1441    |
|       from large pool |     145    |     225    |    1002    |     857    |
|       from small pool |     232    |     238    |     816    |     584    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      26    |      44    |      69    |      43    |
|       from large pool |      23    |      42    |      66    |      43    |
|       from small pool |       3    |       3    |       3    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      24    |      27    |     675    |     651    |
|       from large pool |      16    |      19    |     465    |     449    |
|       from small pool |       8    |      10    |     210    |     202    |
|===========================================================================|

[2021-12-23 11:20:39,909][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:39,909][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:40,513][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 10.92 GiB total capacity; 4.35 GiB already allocated; 54.50 MiB free; 4.41 GiB reserved in total by PyTorch)
[2021-12-23 11:20:40,514][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4458 MB |    5378 MB |   45024 MB |   40565 MB |
|       from large pool |    4455 MB |    5374 MB |   45010 MB |   40555 MB |
|       from small pool |       3 MB |       4 MB |      13 MB |       9 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4458 MB |    5378 MB |   45024 MB |   40565 MB |
|       from large pool |    4455 MB |    5374 MB |   45010 MB |   40555 MB |
|       from small pool |       3 MB |       4 MB |      13 MB |       9 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4518 MB |    5562 MB |    9964 MB |    5446 MB |
|       from large pool |    4514 MB |    5556 MB |    9958 MB |    5444 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   60602 KB |    3031 MB |   34061 MB |   34002 MB |
|       from large pool |   60259 KB |    3031 MB |   34046 MB |   33987 MB |
|       from small pool |     343 KB |       2 MB |      14 MB |      14 MB |
|---------------------------------------------------------------------------|
| Allocations           |     392    |     426    |    2216    |    1824    |
|       from large pool |     208    |     225    |    1232    |    1024    |
|       from small pool |     184    |     238    |     984    |     800    |
|---------------------------------------------------------------------------|
| Active allocs         |     392    |     426    |    2216    |    1824    |
|       from large pool |     208    |     225    |    1232    |    1024    |
|       from small pool |     184    |     238    |     984    |     800    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      18    |      44    |      70    |      52    |
|       from large pool |      16    |      42    |      67    |      51    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      10    |      29    |     837    |     827    |
|       from large pool |       8    |      19    |     555    |     547    |
|       from small pool |       2    |      11    |     282    |     280    |
|===========================================================================|

[2021-12-23 11:20:40,514][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:40,514][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:41,500][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 54.00 MiB (GPU 0; 10.92 GiB total capacity; 4.46 GiB already allocated; 48.50 MiB free; 4.53 GiB reserved in total by PyTorch)
[2021-12-23 11:20:41,500][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4570 MB |    5378 MB |   56771 MB |   52200 MB |
|       from large pool |    4566 MB |    5374 MB |   56756 MB |   52189 MB |
|       from small pool |       3 MB |       4 MB |      15 MB |      11 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4570 MB |    5378 MB |   56771 MB |   52200 MB |
|       from large pool |    4566 MB |    5374 MB |   56756 MB |   52189 MB |
|       from small pool |       3 MB |       4 MB |      15 MB |      11 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4636 MB |    5562 MB |   11166 MB |    6530 MB |
|       from large pool |    4632 MB |    5556 MB |   11160 MB |    6528 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   67056 KB |    3031 MB |   46690 MB |   46624 MB |
|       from large pool |   66777 KB |    3031 MB |   46673 MB |   46608 MB |
|       from small pool |     278 KB |       2 MB |      16 MB |      16 MB |
|---------------------------------------------------------------------------|
| Allocations           |     391    |     426    |    2629    |    2238    |
|       from large pool |     207    |     225    |    1456    |    1249    |
|       from small pool |     184    |     238    |    1173    |     989    |
|---------------------------------------------------------------------------|
| Active allocs         |     391    |     426    |    2629    |    2238    |
|       from large pool |     207    |     225    |    1456    |    1249    |
|       from small pool |     184    |     238    |    1173    |     989    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      72    |      53    |
|       from large pool |      17    |      42    |      69    |      52    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      13    |      29    |    1015    |    1002    |
|       from large pool |      10    |      19    |     656    |     646    |
|       from small pool |       3    |      11    |     359    |     356    |
|===========================================================================|

[2021-12-23 11:20:41,501][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:41,501][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:42,341][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 10.92 GiB total capacity; 4.46 GiB already allocated; 48.50 MiB free; 4.53 GiB reserved in total by PyTorch)
[2021-12-23 11:20:42,343][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4570 MB |    5378 MB |   67874 MB |   63304 MB |
|       from large pool |    4566 MB |    5374 MB |   67857 MB |   63291 MB |
|       from small pool |       3 MB |       4 MB |      16 MB |      12 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4570 MB |    5378 MB |   67874 MB |   63304 MB |
|       from large pool |    4566 MB |    5374 MB |   67857 MB |   63291 MB |
|       from small pool |       3 MB |       4 MB |      16 MB |      12 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4636 MB |    5562 MB |   11166 MB |    6530 MB |
|       from large pool |    4632 MB |    5556 MB |   11160 MB |    6528 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   67273 KB |    3031 MB |   60119 MB |   60054 MB |
|       from large pool |   66980 KB |    3031 MB |   60102 MB |   60036 MB |
|       from small pool |     293 KB |       2 MB |      17 MB |      17 MB |
|---------------------------------------------------------------------------|
| Allocations           |     410    |     426    |    3070    |    2660    |
|       from large pool |     222    |     225    |    1703    |    1481    |
|       from small pool |     188    |     238    |    1367    |    1179    |
|---------------------------------------------------------------------------|
| Active allocs         |     410    |     426    |    3070    |    2660    |
|       from large pool |     222    |     225    |    1703    |    1481    |
|       from small pool |     188    |     238    |    1367    |    1179    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      72    |      53    |
|       from large pool |      17    |      42    |      69    |      52    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      29    |    1219    |    1208    |
|       from large pool |       8    |      19    |     773    |     765    |
|       from small pool |       3    |      11    |     446    |     443    |
|===========================================================================|

[2021-12-23 11:20:42,344][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:42,345][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:43,409][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 10.92 GiB total capacity; 4.20 GiB already allocated; 14.56 MiB free; 4.28 GiB reserved in total by PyTorch)
[2021-12-23 11:20:43,410][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4305 MB |    5378 MB |   79405 MB |   75100 MB |
|       from large pool |    4301 MB |    5374 MB |   79387 MB |   75086 MB |
|       from small pool |       3 MB |       4 MB |      18 MB |      14 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4305 MB |    5378 MB |   79405 MB |   75100 MB |
|       from large pool |    4301 MB |    5374 MB |   79387 MB |   75086 MB |
|       from small pool |       3 MB |       4 MB |      18 MB |      14 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4382 MB |    5562 MB |   13528 MB |    9146 MB |
|       from large pool |    4378 MB |    5556 MB |   13522 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   78497 KB |    3031 MB |   75014 MB |   74938 MB |
|       from large pool |   78213 KB |    3031 MB |   74995 MB |   74918 MB |
|       from small pool |     283 KB |       2 MB |      19 MB |      19 MB |
|---------------------------------------------------------------------------|
| Allocations           |     388    |     426    |    3511    |    3123    |
|       from large pool |     206    |     225    |    1925    |    1719    |
|       from small pool |     182    |     238    |    1586    |    1404    |
|---------------------------------------------------------------------------|
| Active allocs         |     388    |     426    |    3511    |    3123    |
|       from large pool |     206    |     225    |    1925    |    1719    |
|       from small pool |     182    |     238    |    1586    |    1404    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      75    |      56    |
|       from large pool |      17    |      42    |      72    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      14    |      29    |    1425    |    1411    |
|       from large pool |      12    |      19    |     883    |     871    |
|       from small pool |       2    |      11    |     542    |     540    |
|===========================================================================|

[2021-12-23 11:20:43,411][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:43,411][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:43,845][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 10.92 GiB total capacity; 4.37 GiB already allocated; 56.56 MiB free; 4.43 GiB reserved in total by PyTorch)
[2021-12-23 11:20:43,846][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4475 MB |    5378 MB |   88532 MB |   84056 MB |
|       from large pool |    4472 MB |    5374 MB |   88513 MB |   84040 MB |
|       from small pool |       3 MB |       4 MB |      19 MB |      15 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4475 MB |    5378 MB |   88532 MB |   84056 MB |
|       from large pool |    4472 MB |    5374 MB |   88513 MB |   84040 MB |
|       from small pool |       3 MB |       4 MB |      19 MB |      15 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4536 MB |    5562 MB |   13682 MB |    9146 MB |
|       from large pool |    4532 MB |    5556 MB |   13676 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   61545 KB |    3031 MB |   86522 MB |   86462 MB |
|       from large pool |   61018 KB |    3031 MB |   86501 MB |   86442 MB |
|       from small pool |     527 KB |       2 MB |      20 MB |      20 MB |
|---------------------------------------------------------------------------|
| Allocations           |     410    |     426    |    3906    |    3496    |
|       from large pool |     222    |     225    |    2177    |    1955    |
|       from small pool |     188    |     238    |    1729    |    1541    |
|---------------------------------------------------------------------------|
| Active allocs         |     410    |     426    |    3906    |    3496    |
|       from large pool |     222    |     225    |    2177    |    1955    |
|       from small pool |     188    |     238    |    1729    |    1541    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      21    |      44    |      77    |      56    |
|       from large pool |      19    |      42    |      74    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      29    |    1590    |    1579    |
|       from large pool |       9    |      19    |    1005    |     996    |
|       from small pool |       2    |      11    |     585    |     583    |
|===========================================================================|

[2021-12-23 11:20:43,847][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:43,847][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:44,552][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.92 GiB total capacity; 4.38 GiB already allocated; 16.31 MiB free; 4.47 GiB reserved in total by PyTorch)
[2021-12-23 11:20:44,553][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4481 MB |    5378 MB |   98593 MB |   94111 MB |
|       from large pool |    4478 MB |    5374 MB |   98572 MB |   94094 MB |
|       from small pool |       3 MB |       4 MB |      20 MB |      17 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4481 MB |    5378 MB |   98593 MB |   94111 MB |
|       from large pool |    4478 MB |    5374 MB |   98572 MB |   94094 MB |
|       from small pool |       3 MB |       4 MB |      20 MB |      17 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4576 MB |    5562 MB |   13722 MB |    9146 MB |
|       from large pool |    4572 MB |    5556 MB |   13716 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   96327 KB |    3031 MB |   99104 MB |   99010 MB |
|       from large pool |   95946 KB |    3031 MB |   99082 MB |   98988 MB |
|       from small pool |     381 KB |       2 MB |      22 MB |      21 MB |
|---------------------------------------------------------------------------|
| Allocations           |     419    |     426    |    4331    |    3912    |
|       from large pool |     228    |     229    |    2434    |    2206    |
|       from small pool |     191    |     238    |    1897    |    1706    |
|---------------------------------------------------------------------------|
| Active allocs         |     419    |     426    |    4331    |    3912    |
|       from large pool |     228    |     229    |    2434    |    2206    |
|       from small pool |     191    |     238    |    1897    |    1706    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      22    |      44    |      78    |      56    |
|       from large pool |      20    |      42    |      75    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      14    |      29    |    1786    |    1772    |
|       from large pool |      11    |      19    |    1133    |    1122    |
|       from small pool |       3    |      11    |     653    |     650    |
|===========================================================================|

[2021-12-23 11:20:44,554][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:44,554][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:45,387][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 10.92 GiB total capacity; 4.39 GiB already allocated; 16.50 MiB free; 4.47 GiB reserved in total by PyTorch)
[2021-12-23 11:20:45,388][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4495 MB |    5378 MB |  109750 MB |  105254 MB |
|       from large pool |    4491 MB |    5374 MB |  109728 MB |  105236 MB |
|       from small pool |       3 MB |       4 MB |      22 MB |      18 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4495 MB |    5378 MB |  109750 MB |  105254 MB |
|       from large pool |    4491 MB |    5374 MB |  109728 MB |  105236 MB |
|       from small pool |       3 MB |       4 MB |      22 MB |      18 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4576 MB |    5562 MB |   13722 MB |    9146 MB |
|       from large pool |    4572 MB |    5556 MB |   13716 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   82233 KB |    3031 MB |  113058 MB |  112978 MB |
|       from large pool |   81962 KB |    3031 MB |  113035 MB |  112955 MB |
|       from small pool |     271 KB |       2 MB |      23 MB |      23 MB |
|---------------------------------------------------------------------------|
| Allocations           |     408    |     426    |    4770    |    4362    |
|       from large pool |     220    |     229    |    2679    |    2459    |
|       from small pool |     188    |     238    |    2091    |    1903    |
|---------------------------------------------------------------------------|
| Active allocs         |     408    |     426    |    4770    |    4362    |
|       from large pool |     220    |     229    |    2679    |    2459    |
|       from small pool |     188    |     238    |    2091    |    1903    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      22    |      44    |      78    |      56    |
|       from large pool |      20    |      42    |      75    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      17    |      29    |    1971    |    1954    |
|       from large pool |      14    |      19    |    1261    |    1247    |
|       from small pool |       3    |      11    |     710    |     707    |
|===========================================================================|

[2021-12-23 11:20:45,389][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:45,390][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:46,150][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 10.92 GiB total capacity; 4.35 GiB already allocated; 24.50 MiB free; 4.47 GiB reserved in total by PyTorch)
[2021-12-23 11:20:46,150][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4453 MB |    5378 MB |  119891 MB |  115438 MB |
|       from large pool |    4449 MB |    5374 MB |  119868 MB |  115418 MB |
|       from small pool |       3 MB |       4 MB |      23 MB |      20 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4453 MB |    5378 MB |  119891 MB |  115438 MB |
|       from large pool |    4449 MB |    5374 MB |  119868 MB |  115418 MB |
|       from small pool |       3 MB |       4 MB |      23 MB |      20 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4576 MB |    5562 MB |   13722 MB |    9146 MB |
|       from large pool |    4572 MB |    5556 MB |   13716 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  125797 KB |    3031 MB |  126782 MB |  126659 MB |
|       from large pool |  125382 KB |    3031 MB |  126756 MB |  126634 MB |
|       from small pool |     415 KB |       2 MB |      25 MB |      24 MB |
|---------------------------------------------------------------------------|
| Allocations           |     403    |     426    |    5170    |    4767    |
|       from large pool |     216    |     229    |    2916    |    2700    |
|       from small pool |     187    |     238    |    2254    |    2067    |
|---------------------------------------------------------------------------|
| Active allocs         |     403    |     426    |    5170    |    4767    |
|       from large pool |     216    |     229    |    2916    |    2700    |
|       from small pool |     187    |     238    |    2254    |    2067    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      22    |      44    |      78    |      56    |
|       from large pool |      20    |      42    |      75    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      16    |      29    |    2147    |    2131    |
|       from large pool |      13    |      19    |    1374    |    1361    |
|       from small pool |       3    |      11    |     773    |     770    |
|===========================================================================|

[2021-12-23 11:20:46,151][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:46,151][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:46,942][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 110.00 MiB (GPU 0; 10.92 GiB total capacity; 4.40 GiB already allocated; 76.50 MiB free; 4.47 GiB reserved in total by PyTorch)
[2021-12-23 11:20:46,943][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4505 MB |    5378 MB |  131655 MB |  127150 MB |
|       from large pool |    4501 MB |    5374 MB |  131630 MB |  127128 MB |
|       from small pool |       3 MB |       4 MB |      25 MB |      21 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4505 MB |    5378 MB |  131655 MB |  127150 MB |
|       from large pool |    4501 MB |    5374 MB |  131630 MB |  127128 MB |
|       from small pool |       3 MB |       4 MB |      25 MB |      21 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4576 MB |    5562 MB |   13722 MB |    9146 MB |
|       from large pool |    4572 MB |    5556 MB |   13716 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   72484 KB |    3031 MB |  139492 MB |  139421 MB |
|       from large pool |   72231 KB |    3031 MB |  139466 MB |  139395 MB |
|       from small pool |     252 KB |       2 MB |      26 MB |      26 MB |
|---------------------------------------------------------------------------|
| Allocations           |     392    |     426    |    5600    |    5208    |
|       from large pool |     208    |     229    |    3141    |    2933    |
|       from small pool |     184    |     238    |    2459    |    2275    |
|---------------------------------------------------------------------------|
| Active allocs         |     392    |     426    |    5600    |    5208    |
|       from large pool |     208    |     229    |    3141    |    2933    |
|       from small pool |     184    |     238    |    2459    |    2275    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      22    |      44    |      78    |      56    |
|       from large pool |      20    |      42    |      75    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      14    |      29    |    2354    |    2340    |
|       from large pool |      11    |      19    |    1495    |    1484    |
|       from small pool |       3    |      11    |     859    |     856    |
|===========================================================================|

[2021-12-23 11:20:46,944][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:46,944][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:47,896][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 50.00 MiB (GPU 0; 10.92 GiB total capacity; 4.43 GiB already allocated; 12.50 MiB free; 4.48 GiB reserved in total by PyTorch)
[2021-12-23 11:20:47,898][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4533 MB |    5378 MB |  142841 MB |  138307 MB |
|       from large pool |    4530 MB |    5374 MB |  142814 MB |  138284 MB |
|       from small pool |       3 MB |       4 MB |      27 MB |      23 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4533 MB |    5378 MB |  142841 MB |  138307 MB |
|       from large pool |    4530 MB |    5374 MB |  142814 MB |  138284 MB |
|       from small pool |       3 MB |       4 MB |      27 MB |      23 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4590 MB |    5562 MB |   13736 MB |    9146 MB |
|       from large pool |    4586 MB |    5556 MB |   13730 MB |    9144 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   57676 KB |    3031 MB |  152417 MB |  152361 MB |
|       from large pool |   57333 KB |    3031 MB |  152389 MB |  152333 MB |
|       from small pool |     343 KB |       2 MB |      28 MB |      28 MB |
|---------------------------------------------------------------------------|
| Allocations           |     391    |     426    |    5997    |    5606    |
|       from large pool |     207    |     229    |    3370    |    3163    |
|       from small pool |     184    |     238    |    2627    |    2443    |
|---------------------------------------------------------------------------|
| Active allocs         |     391    |     426    |    5997    |    5606    |
|       from large pool |     207    |     229    |    3370    |    3163    |
|       from small pool |     184    |     238    |    2627    |    2443    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      23    |      44    |      79    |      56    |
|       from large pool |      21    |      42    |      76    |      55    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      16    |      29    |    2520    |    2504    |
|       from large pool |      13    |      19    |    1617    |    1604    |
|       from small pool |       3    |      11    |     903    |     900    |
|===========================================================================|

[2021-12-23 11:20:47,899][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:47,899][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:48,789][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 10.92 GiB total capacity; 4.25 GiB already allocated; 18.50 MiB free; 4.30 GiB reserved in total by PyTorch)
[2021-12-23 11:20:48,790][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4350 MB |    5378 MB |  154415 MB |  150064 MB |
|       from large pool |    4346 MB |    5374 MB |  154386 MB |  150040 MB |
|       from small pool |       3 MB |       4 MB |      28 MB |      24 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4350 MB |    5378 MB |  154415 MB |  150064 MB |
|       from large pool |    4346 MB |    5374 MB |  154386 MB |  150040 MB |
|       from small pool |       3 MB |       4 MB |      28 MB |      24 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4402 MB |    5562 MB |   16124 MB |   11722 MB |
|       from large pool |    4398 MB |    5556 MB |   16118 MB |   11720 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   53067 KB |    3031 MB |  168213 MB |  168161 MB |
|       from large pool |   52752 KB |    3031 MB |  168183 MB |  168131 MB |
|       from small pool |     315 KB |       2 MB |      29 MB |      29 MB |
|---------------------------------------------------------------------------|
| Allocations           |     374    |     426    |    6384    |    6010    |
|       from large pool |     194    |     229    |    3573    |    3379    |
|       from small pool |     180    |     238    |    2811    |    2631    |
|---------------------------------------------------------------------------|
| Active allocs         |     374    |     426    |    6384    |    6010    |
|       from large pool |     194    |     229    |    3573    |    3379    |
|       from small pool |     180    |     238    |    2811    |    2631    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      81    |      62    |
|       from large pool |      17    |      42    |      78    |      61    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       9    |      29    |    2698    |    2689    |
|       from large pool |       7    |      19    |    1719    |    1712    |
|       from small pool |       2    |      11    |     979    |     977    |
|===========================================================================|

[2021-12-23 11:20:48,791][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:48,791][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:49,400][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 10.92 GiB total capacity; 4.26 GiB already allocated; 18.50 MiB free; 4.30 GiB reserved in total by PyTorch)
[2021-12-23 11:20:49,401][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4360 MB |    5378 MB |  165043 MB |  160682 MB |
|       from large pool |    4357 MB |    5374 MB |  165012 MB |  160655 MB |
|       from small pool |       3 MB |       4 MB |      30 MB |      26 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4360 MB |    5378 MB |  165043 MB |  160682 MB |
|       from large pool |    4357 MB |    5374 MB |  165012 MB |  160655 MB |
|       from small pool |       3 MB |       4 MB |      30 MB |      26 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4402 MB |    5562 MB |   16124 MB |   11722 MB |
|       from large pool |    4398 MB |    5556 MB |   16118 MB |   11720 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   42169 KB |    3031 MB |  180815 MB |  180774 MB |
|       from large pool |   41703 KB |    3031 MB |  180784 MB |  180743 MB |
|       from small pool |     466 KB |       2 MB |      31 MB |      31 MB |
|---------------------------------------------------------------------------|
| Allocations           |     359    |     426    |    6717    |    6358    |
|       from large pool |     183    |     229    |    3756    |    3573    |
|       from small pool |     176    |     238    |    2961    |    2785    |
|---------------------------------------------------------------------------|
| Active allocs         |     359    |     426    |    6717    |    6358    |
|       from large pool |     183    |     229    |    3756    |    3573    |
|       from small pool |     176    |     238    |    2961    |    2785    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      81    |      62    |
|       from large pool |      17    |      42    |      78    |      61    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      11    |      29    |    2837    |    2826    |
|       from large pool |       9    |      19    |    1813    |    1804    |
|       from small pool |       2    |      11    |    1024    |    1022    |
|===========================================================================|

[2021-12-23 11:20:49,401][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:49,401][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:50,148][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 10.92 GiB total capacity; 4.20 GiB already allocated; 18.50 MiB free; 4.30 GiB reserved in total by PyTorch)
[2021-12-23 11:20:50,148][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    4303 MB |    5378 MB |  176025 MB |  171722 MB |
|       from large pool |    4300 MB |    5374 MB |  175994 MB |  171694 MB |
|       from small pool |       3 MB |       4 MB |      31 MB |      27 MB |
|---------------------------------------------------------------------------|
| Active memory         |    4303 MB |    5378 MB |  176025 MB |  171722 MB |
|       from large pool |    4300 MB |    5374 MB |  175994 MB |  171694 MB |
|       from small pool |       3 MB |       4 MB |      31 MB |      27 MB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4402 MB |    5562 MB |   16124 MB |   11722 MB |
|       from large pool |    4398 MB |    5556 MB |   16118 MB |   11720 MB |
|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  100663 KB |    3031 MB |  195691 MB |  195592 MB |
|       from large pool |  100352 KB |    3031 MB |  195658 MB |  195560 MB |
|       from small pool |     311 KB |       2 MB |      32 MB |      32 MB |
|---------------------------------------------------------------------------|
| Allocations           |     393    |     426    |    7148    |    6755    |
|       from large pool |     209    |     229    |    3982    |    3773    |
|       from small pool |     184    |     238    |    3166    |    2982    |
|---------------------------------------------------------------------------|
| Active allocs         |     393    |     426    |    7148    |    6755    |
|       from large pool |     209    |     229    |    3982    |    3773    |
|       from small pool |     184    |     238    |    3166    |    2982    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      19    |      44    |      81    |      62    |
|       from large pool |      17    |      42    |      78    |      61    |
|       from small pool |       2    |       3    |       3    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      13    |      29    |    3020    |    3007    |
|       from large pool |      10    |      19    |    1916    |    1906    |
|       from small pool |       3    |      11    |    1104    |    1101    |
|===========================================================================|

[2021-12-23 11:20:50,149][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-23 11:20:50,149][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-23 11:20:50,152][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2021-12-23 11:21:24,903][valid][INFO] - {"epoch": 1, "valid_loss": "780.647", "valid_ntokens": "852.705", "valid_nsentences": "24.5455", "valid_nll_loss": "22.471", "valid_uer": "313.383", "valid_wer": "2651.48", "valid_raw_wer": "2651.48", "valid_wps": "1113.2", "valid_wpb": "852.7", "valid_bsz": "24.5", "valid_num_updates": "0"}
[2021-12-23 11:21:24,907][fairseq.checkpoint_utils][INFO] - Preparing to save checkpoint for epoch 1 @ 0 updates
[2021-12-23 11:21:24,908][fairseq.trainer][INFO] - Saving checkpoint to /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_best.pt
[2021-12-23 11:21:25,605][fairseq.trainer][INFO] - Finished saving checkpoint to /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_best.pt
[2021-12-23 11:21:25,973][fairseq.checkpoint_utils][INFO] - Saved checkpoint /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_best.pt (epoch 1 @ 0 updates, score 2651.481) (writing took 1.0661228249955457 seconds)
[2021-12-23 11:21:25,973][fairseq_cli.train][INFO] - end of epoch 1 (average epoch stats below)
[2021-12-23 11:21:25,976][train][INFO] - {"epoch": 1, "train_lr": "5e-07", "train_train_wall": "36", "train_wall": "55"}
[2021-12-23 11:21:26,028][fairseq.trainer][INFO] - begin training epoch 2
[2021-12-23 11:21:26,029][fairseq_cli.train][INFO] - Start iterating over samples
