[2021-12-22 12:12:25,997][fairseq_cli.train][INFO] - {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/logs/finetuning/tensorboard_2021-12-22_12-12-21', 'wandb_project': 'ieee_worksop', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 6, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3200000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3200000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 500, 'max_update': 20000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': True, 'update_freq': [24], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning', 'restore_file': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'wer', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec_ctc', 'w2v_path': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/pretraining/CLSRIL-23.pt', 'no_pretrained_weights': False, 'dropout_input': 0.0, 'final_dropout': 0.0, 'dropout': 0.0, 'attention_dropout': 0.0, 'activation_dropout': 0.1, 'apply_mask': True, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'freeze_finetune_updates': 0, 'feature_grad_mult': 0.0, 'layerdrop': 0.05, 'normalize': False, 'data': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/data/finetuning', 'w2v_args': None, 'mask_min_space': 1, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768}, 'task': {'_name': 'audio_pretraining', 'data': '/home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/data/finetuning', 'labels': 'ltr', 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_sample_size': None, 'min_sample_size': None, 'eval_wer': False, 'eval_wer_config': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_wer_tokenizer': None, 'eval_wer_post_process': 'letter', 'autoregressive': False, 'num_batch_buckets': 0, 'precompute_mask_indices': False, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 64, 'mask_channel_prob': 0.5, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'encoder_embed_dim': 768, 'tpu': False}, 'criterion': {'_name': 'ctc', 'zero_infinity': True, 'sentence_avg': True, 'post_process': 'letter', 'wer_kenlm_model': None, 'wer_lexicon': None, 'wer_lm_weight': 2.0, 'wer_word_score': -1.0, 'wer_args': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'tri_stage', 'warmup_steps': 0, 'hold_steps': 0, 'decay_steps': 0, 'phase_ratio': [0.1, 0.4, 0.5], 'init_lr_scale': 0.01, 'final_lr_scale': 0.05, 'max_update': 20000, 'lr': [5e-05]}, 'scoring': None, 'bpe': None, 'tokenizer': None, 'job_logging_cfg': {'version': 1, 'formatters': {'simple': {'format': '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'}}, 'handlers': {'console': {'class': 'logging.StreamHandler', 'formatter': 'simple', 'stream': 'ext://sys.stdout'}, 'file': {'class': 'logging.FileHandler', 'formatter': 'simple', 'filename': 'hydra_train.log'}}, 'root': {'level': 'INFO', 'handlers': ['console', 'file']}, 'disable_existing_loggers': False}}
[2021-12-22 12:12:34,738][fairseq_cli.train][INFO] - Wav2VecCtc(
  (w2v_encoder): Wav2VecEncoder(
    (w2v_model): Wav2Vec2Model(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU()
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU()
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (quantizer): None
      (project_q): None
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU()
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.0, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.0, inplace=False)
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (final_dropout): Dropout(p=0.0, inplace=False)
    (proj): Linear(in_features=768, out_features=8, bias=True)
  )
)
[2021-12-22 12:12:34,745][fairseq_cli.train][INFO] - task: AudioPretrainingTask
[2021-12-22 12:12:34,746][fairseq_cli.train][INFO] - model: Wav2VecCtc
[2021-12-22 12:12:34,746][fairseq_cli.train][INFO] - criterion: CtcCriterion
[2021-12-22 12:12:34,750][fairseq_cli.train][INFO] - num. shared model params: 94,377,864 (num. trained: 94,377,864)
[2021-12-22 12:12:34,754][fairseq_cli.train][INFO] - num. expert model params: 0 (num. trained: 0)
[2021-12-22 12:12:34,767][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 1047, skipped 0 samples
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
[2021-12-22 12:12:51,016][fairseq.trainer][INFO] - detected shared parameter: w2v_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias <- w2v_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
[2021-12-22 12:12:51,017][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2021-12-22 12:12:51,017][fairseq.utils][INFO] - rank   0: capabilities =  6.1  ; total memory = 10.915 GB ; name = NVIDIA GeForce GTX 1080 Ti              
[2021-12-22 12:12:51,017][fairseq.utils][INFO] - ***********************CUDA enviroments for all 1 workers***********************
[2021-12-22 12:12:51,018][fairseq_cli.train][INFO] - training on 1 devices (GPUs/TPUs)
[2021-12-22 12:12:51,018][fairseq_cli.train][INFO] - max tokens per device = 3200000 and max sentences per device = None
[2021-12-22 12:12:51,019][fairseq.trainer][INFO] - Preparing to load checkpoint /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt
[2021-12-22 12:12:57,690][fairseq.trainer][INFO] - NOTE: your device does NOT support faster training with --fp16, please switch to FP32 which is likely to be faster
[2021-12-22 12:12:58,752][fairseq.trainer][INFO] - Loaded checkpoint /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/checkpoints/finetuning/checkpoint_last.pt (epoch 447 @ 5791 updates)
[2021-12-22 12:12:58,794][fairseq.trainer][INFO] - loading train data for epoch 447
[2021-12-22 12:12:58,819][fairseq.data.audio.raw_audio_dataset][INFO] - loaded 8225, skipped 0 samples
wandb: Currently logged in as: vaishshells (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.12.9 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.5
wandb: Syncing run finetuning
wandb: ⭐️ View project at https://wandb.ai/vaishshells/ieee_worksop
wandb: 🚀 View run at https://wandb.ai/vaishshells/ieee_worksop/runs/4dt82hf2
wandb: Run data is saved locally in /home/orchids/Documents/BTP_Vaishnavi_Joshitha/NewTrial/vakyansh-wav2vec2-experimentation/scripts/finetuning/outputs/2021-12-22/12-12-25/wandb/run-20211222_121302-4dt82hf2
wandb: Run `wandb offline` to turn off syncing.

[2021-12-22 12:13:11,412][fairseq.trainer][INFO] - begin training epoch 447
[2021-12-22 12:13:11,520][fairseq_cli.train][INFO] - Start iterating over samples
[2021-12-22 12:13:24,373][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 626.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 115.19 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:13:24,400][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1637 MB |    1649 MB |    1649 MB |   12500 KB |
|       from large pool |    1633 MB |    1646 MB |    1646 MB |   12500 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       0 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1637 MB |    1649 MB |    1649 MB |   12500 KB |
|       from large pool |    1633 MB |    1646 MB |    1646 MB |   12500 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       0 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1672 MB |       0 B  |
|       from large pool |    1668 MB |    1668 MB |    1668 MB |       0 B  |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   35832 KB |   35832 KB |  210048 KB |  174215 KB |
|       from large pool |   34826 KB |   34826 KB |  206986 KB |  172160 KB |
|       from small pool |    1006 KB |    2038 KB |    3062 KB |    2055 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     223    |       1    |
|       from large pool |      83    |      84    |      84    |       1    |
|       from small pool |     139    |     139    |     139    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     223    |       1    |
|       from large pool |      83    |      84    |      84    |       1    |
|       from small pool |     139    |     139    |     139    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      17    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       7    |       7    |      18    |      11    |
|       from large pool |       6    |       6    |      16    |      10    |
|       from small pool |       1    |       2    |       2    |       1    |
|===========================================================================|

[2021-12-22 12:13:24,401][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:24,401][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:25,669][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 476.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 115.19 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:13:25,670][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1634 MB |    1649 MB |    1665 MB |   31637 KB |
|       from large pool |    1631 MB |    1646 MB |    1662 MB |   31632 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       5 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1634 MB |    1649 MB |    1665 MB |   31637 KB |
|       from large pool |    1631 MB |    1646 MB |    1662 MB |   31632 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       5 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1672 MB |       0 B  |
|       from large pool |    1668 MB |    1668 MB |    1668 MB |       0 B  |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   38322 KB |   38322 KB |  238660 KB |  200337 KB |
|       from large pool |   37315 KB |   37315 KB |  235593 KB |  198277 KB |
|       from small pool |    1007 KB |    2038 KB |    3067 KB |    2060 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     229    |       7    |
|       from large pool |      83    |      84    |      87    |       4    |
|       from small pool |     139    |     139    |     142    |       3    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     229    |       7    |
|       from large pool |      83    |      84    |      87    |       4    |
|       from small pool |     139    |     139    |     142    |       3    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      17    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       8    |      21    |      13    |
|       from large pool |       7    |       7    |      18    |      11    |
|       from small pool |       1    |       2    |       3    |       2    |
|===========================================================================|

[2021-12-22 12:13:25,671][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:25,671][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:30,698][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 536.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 115.19 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:13:30,699][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1683 MB |   49474 KB |
|       from large pool |    1632 MB |    1646 MB |    1680 MB |   49464 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       9 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1683 MB |   49474 KB |
|       from large pool |    1632 MB |    1646 MB |    1680 MB |   49464 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |       9 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1672 MB |       0 B  |
|       from large pool |    1668 MB |    1668 MB |    1668 MB |       0 B  |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37430 KB |   38323 KB |  266872 KB |  229442 KB |
|       from large pool |   36423 KB |   37315 KB |  263800 KB |  227377 KB |
|       from small pool |    1007 KB |    2038 KB |    3071 KB |    2064 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     235    |      13    |
|       from large pool |      83    |      84    |      90    |       7    |
|       from small pool |     139    |     139    |     145    |       6    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     235    |      13    |
|       from large pool |      83    |      84    |      90    |       7    |
|       from small pool |     139    |     139    |     145    |       6    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      17    |       0    |
|       from large pool |      15    |      15    |      15    |       0    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      24    |      16    |
|       from large pool |       7    |       7    |      20    |      13    |
|       from small pool |       1    |       2    |       4    |       3    |
|===========================================================================|

[2021-12-22 12:13:30,700][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:30,700][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:35,777][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 394.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 155.19 MiB free; 1.61 GiB reserved in total by PyTorch)
[2021-12-22 12:13:35,777][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1633 MB |    1649 MB |    1697 MB |   65358 KB |
|       from large pool |    1630 MB |    1646 MB |    1694 MB |   65344 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      14 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1633 MB |    1649 MB |    1697 MB |   65358 KB |
|       from large pool |    1630 MB |    1646 MB |    1694 MB |   65344 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      14 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1652 MB |    1672 MB |    1672 MB |   20480 KB |
|       from large pool |    1648 MB |    1668 MB |    1668 MB |   20480 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   19024 KB |   38323 KB |  277406 KB |  258382 KB |
|       from large pool |   18016 KB |   37315 KB |  274330 KB |  256314 KB |
|       from small pool |    1008 KB |    2038 KB |    3076 KB |    2068 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     241    |      19    |
|       from large pool |      83    |      84    |      93    |      10    |
|       from small pool |     139    |     139    |     148    |       9    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     241    |      19    |
|       from large pool |      83    |      84    |      93    |      10    |
|       from small pool |     139    |     139    |     148    |       9    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      16    |      17    |      17    |       1    |
|       from large pool |      14    |      15    |      15    |       1    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       9    |      26    |      20    |
|       from large pool |       5    |       7    |      21    |      16    |
|       from small pool |       1    |       2    |       5    |       4    |
|===========================================================================|

[2021-12-22 12:13:35,778][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:35,778][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:41,380][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 538.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 115.19 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:13:41,381][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1715 MB |   82053 KB |
|       from large pool |    1632 MB |    1646 MB |    1712 MB |   82036 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      17 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1715 MB |   82053 KB |
|       from large pool |    1632 MB |    1646 MB |    1712 MB |   82036 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      17 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1692 MB |   20480 KB |
|       from large pool |    1668 MB |    1668 MB |    1688 MB |   20480 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37400 KB |   38323 KB |  309211 KB |  271811 KB |
|       from large pool |   36393 KB |   37315 KB |  306132 KB |  269739 KB |
|       from small pool |    1007 KB |    2038 KB |    3079 KB |    2072 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     247    |      25    |
|       from large pool |      83    |      84    |      96    |      13    |
|       from small pool |     139    |     139    |     151    |      12    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     247    |      25    |
|       from large pool |      83    |      84    |      96    |      13    |
|       from small pool |     139    |     139    |     151    |      12    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      18    |       1    |
|       from large pool |      15    |      15    |      16    |       1    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      30    |      22    |
|       from large pool |       7    |       7    |      24    |      17    |
|       from small pool |       1    |       2    |       6    |       5    |
|===========================================================================|

[2021-12-22 12:13:41,382][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:41,382][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:46,166][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 342.00 MiB (GPU 0; 10.92 GiB total capacity; 1.59 GiB already allocated; 155.19 MiB free; 1.61 GiB reserved in total by PyTorch)
[2021-12-22 12:13:46,166][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1632 MB |    1649 MB |    1727 MB |   96923 KB |
|       from large pool |    1629 MB |    1646 MB |    1724 MB |   96901 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      22 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1632 MB |    1649 MB |    1727 MB |   96923 KB |
|       from large pool |    1629 MB |    1646 MB |    1724 MB |   96901 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      22 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1652 MB |    1672 MB |    1692 MB |   40960 KB |
|       from large pool |    1648 MB |    1668 MB |    1688 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   19547 KB |   38323 KB |  318711 KB |  299164 KB |
|       from large pool |   18539 KB |   37315 KB |  315627 KB |  297088 KB |
|       from small pool |    1008 KB |    2038 KB |    3084 KB |    2076 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     253    |      31    |
|       from large pool |      83    |      84    |      99    |      16    |
|       from small pool |     139    |     139    |     154    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     253    |      31    |
|       from large pool |      83    |      84    |      99    |      16    |
|       from small pool |     139    |     139    |     154    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      16    |      17    |      18    |       2    |
|       from large pool |      14    |      15    |      16    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       9    |      32    |      26    |
|       from large pool |       5    |       7    |      25    |      20    |
|       from small pool |       1    |       2    |       7    |       6    |
|===========================================================================|

[2021-12-22 12:13:46,167][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:46,167][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:13:55,937][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 518.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 114.25 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:13:55,938][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1745 MB |  112705 KB |
|       from large pool |    1632 MB |    1646 MB |    1742 MB |  112680 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      25 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1745 MB |  112705 KB |
|       from large pool |    1632 MB |    1646 MB |    1742 MB |  112680 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      25 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37692 KB |   38323 KB |  349798 KB |  312106 KB |
|       from large pool |   36685 KB |   37315 KB |  346711 KB |  310025 KB |
|       from small pool |    1007 KB |    2038 KB |    3087 KB |    2080 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     259    |      37    |
|       from large pool |      83    |      84    |     102    |      19    |
|       from small pool |     139    |     139    |     157    |      18    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     259    |      37    |
|       from large pool |      83    |      84    |     102    |      19    |
|       from small pool |     139    |     139    |     157    |      18    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      36    |      28    |
|       from large pool |       7    |       7    |      28    |      21    |
|       from small pool |       1    |       2    |       8    |       7    |
|===========================================================================|

[2021-12-22 12:13:55,939][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:13:55,939][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:00,850][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 622.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 114.19 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:00,851][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1637 MB |    1649 MB |    1766 MB |  132912 KB |
|       from large pool |    1634 MB |    1646 MB |    1763 MB |  132882 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      30 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1637 MB |    1649 MB |    1766 MB |  132912 KB |
|       from large pool |    1634 MB |    1646 MB |    1763 MB |  132882 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      30 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   35802 KB |   38323 KB |  379090 KB |  343288 KB |
|       from large pool |   34796 KB |   37315 KB |  375998 KB |  341202 KB |
|       from small pool |    1006 KB |    2038 KB |    3092 KB |    2086 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     265    |      43    |
|       from large pool |      83    |      84    |     105    |      22    |
|       from small pool |     139    |     139    |     160    |      21    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     265    |      43    |
|       from large pool |      83    |      84    |     105    |      22    |
|       from small pool |     139    |     139    |     160    |      21    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       7    |       9    |      39    |      32    |
|       from large pool |       6    |       7    |      30    |      24    |
|       from small pool |       1    |       2    |       9    |       8    |
|===========================================================================|

[2021-12-22 12:14:00,852][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:00,853][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:07,554][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 590.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 112.56 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:07,555][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1636 MB |    1649 MB |    1786 MB |  154357 KB |
|       from large pool |    1633 MB |    1646 MB |    1783 MB |  154321 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      35 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1636 MB |    1649 MB |    1786 MB |  154357 KB |
|       from large pool |    1633 MB |    1646 MB |    1783 MB |  154321 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      35 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   36614 KB |   38323 KB |  408901 KB |  372287 KB |
|       from large pool |   35607 KB |   37315 KB |  405804 KB |  370196 KB |
|       from small pool |    1006 KB |    2038 KB |    3097 KB |    2091 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     271    |      49    |
|       from large pool |      83    |      84    |     108    |      25    |
|       from small pool |     139    |     139    |     163    |      24    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     271    |      49    |
|       from large pool |      83    |      84    |     108    |      25    |
|       from small pool |     139    |     139    |     163    |      24    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      42    |      34    |
|       from large pool |       7    |       7    |      32    |      25    |
|       from small pool |       1    |       2    |      10    |       9    |
|===========================================================================|

[2021-12-22 12:14:07,555][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:07,556][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:13,717][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 574.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 64.50 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:13,718][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1636 MB |    1649 MB |    1806 MB |  174662 KB |
|       from large pool |    1632 MB |    1646 MB |    1803 MB |  174622 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      40 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1636 MB |    1649 MB |    1806 MB |  174662 KB |
|       from large pool |    1632 MB |    1646 MB |    1803 MB |  174622 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      40 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   36859 KB |   38323 KB |  438063 KB |  401204 KB |
|       from large pool |   35853 KB |   37315 KB |  434961 KB |  399108 KB |
|       from small pool |    1006 KB |    2038 KB |    3102 KB |    2096 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     277    |      55    |
|       from large pool |      83    |      84    |     111    |      28    |
|       from small pool |     139    |     139    |     166    |      27    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     277    |      55    |
|       from large pool |      83    |      84    |     111    |      28    |
|       from small pool |     139    |     139    |     166    |      27    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      45    |      37    |
|       from large pool |       7    |       7    |      34    |      27    |
|       from small pool |       1    |       2    |      11    |      10    |
|===========================================================================|

[2021-12-22 12:14:13,718][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:13,718][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:19,733][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 522.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 52.38 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:19,734][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1824 MB |  193687 KB |
|       from large pool |    1632 MB |    1646 MB |    1821 MB |  193642 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      45 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1824 MB |  193687 KB |
|       from large pool |    1632 MB |    1646 MB |    1821 MB |  193642 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      45 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37636 KB |   38323 KB |  466626 KB |  428990 KB |
|       from large pool |   36629 KB |   37315 KB |  463518 KB |  426889 KB |
|       from small pool |    1007 KB |    2038 KB |    3107 KB |    2100 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     283    |      61    |
|       from large pool |      83    |      84    |     114    |      31    |
|       from small pool |     139    |     139    |     169    |      30    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     283    |      61    |
|       from large pool |      83    |      84    |     114    |      31    |
|       from small pool |     139    |     139    |     169    |      30    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      48    |      40    |
|       from large pool |       7    |       7    |      36    |      29    |
|       from small pool |       1    |       2    |      12    |      11    |
|===========================================================================|

[2021-12-22 12:14:19,734][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:19,735][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:25,073][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 470.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 61.25 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:25,074][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1634 MB |    1649 MB |    1840 MB |  210881 KB |
|       from large pool |    1631 MB |    1646 MB |    1837 MB |  210831 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      50 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1634 MB |    1649 MB |    1840 MB |  210881 KB |
|       from large pool |    1631 MB |    1646 MB |    1837 MB |  210831 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      50 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   38428 KB |   38428 KB |  494402 KB |  455974 KB |
|       from large pool |   37420 KB |   37420 KB |  491290 KB |  453869 KB |
|       from small pool |    1007 KB |    2038 KB |    3112 KB |    2104 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     289    |      67    |
|       from large pool |      83    |      84    |     117    |      34    |
|       from small pool |     139    |     139    |     172    |      33    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     289    |      67    |
|       from large pool |      83    |      84    |     117    |      34    |
|       from small pool |     139    |     139    |     172    |      33    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      51    |      43    |
|       from large pool |       7    |       7    |      38    |      31    |
|       from small pool |       1    |       2    |      13    |      12    |
|===========================================================================|

[2021-12-22 12:14:25,074][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:25,074][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:29,151][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 460.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 61.25 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:29,152][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1634 MB |    1649 MB |    1856 MB |  227102 KB |
|       from large pool |    1631 MB |    1646 MB |    1853 MB |  227048 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      54 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1634 MB |    1649 MB |    1856 MB |  227102 KB |
|       from large pool |    1631 MB |    1646 MB |    1853 MB |  227048 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      54 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1712 MB |   40960 KB |
|       from large pool |    1668 MB |    1668 MB |    1708 MB |   40960 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   38563 KB |   38563 KB |  521823 KB |  483260 KB |
|       from large pool |   37555 KB |   37555 KB |  518707 KB |  481152 KB |
|       from small pool |    1007 KB |    2038 KB |    3116 KB |    2108 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     295    |      73    |
|       from large pool |      83    |      84    |     120    |      37    |
|       from small pool |     139    |     139    |     175    |      36    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     295    |      73    |
|       from large pool |      83    |      84    |     120    |      37    |
|       from small pool |     139    |     139    |     175    |      36    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      19    |       2    |
|       from large pool |      15    |      15    |      17    |       2    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      54    |      46    |
|       from large pool |       7    |       7    |      40    |      33    |
|       from small pool |       1    |       2    |      14    |      13    |
|===========================================================================|

[2021-12-22 12:14:29,154][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:29,154][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:35,387][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 370.00 MiB (GPU 0; 10.92 GiB total capacity; 1.59 GiB already allocated; 101.00 MiB free; 1.61 GiB reserved in total by PyTorch)
[2021-12-22 12:14:35,387][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1633 MB |    1649 MB |    1868 MB |  241369 KB |
|       from large pool |    1630 MB |    1646 MB |    1865 MB |  241311 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      58 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1633 MB |    1649 MB |    1868 MB |  241369 KB |
|       from large pool |    1630 MB |    1646 MB |    1865 MB |  241311 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      58 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1652 MB |    1672 MB |    1712 MB |   61440 KB |
|       from large pool |    1648 MB |    1668 MB |    1708 MB |   61440 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   19267 KB |   38563 KB |  531495 KB |  512228 KB |
|       from large pool |   18259 KB |   37555 KB |  528375 KB |  510116 KB |
|       from small pool |    1008 KB |    2038 KB |    3120 KB |    2112 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     301    |      79    |
|       from large pool |      83    |      84    |     123    |      40    |
|       from small pool |     139    |     139    |     178    |      39    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     301    |      79    |
|       from large pool |      83    |      84    |     123    |      40    |
|       from small pool |     139    |     139    |     178    |      39    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      16    |      17    |      19    |       3    |
|       from large pool |      14    |      15    |      17    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       9    |      56    |      50    |
|       from large pool |       5    |       7    |      41    |      36    |
|       from small pool |       1    |       2    |      15    |      14    |
|===========================================================================|

[2021-12-22 12:14:35,388][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:35,388][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:39,422][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 428.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 113.25 MiB free; 1.61 GiB reserved in total by PyTorch)
[2021-12-22 12:14:39,423][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1634 MB |    1649 MB |    1884 MB |  255641 KB |
|       from large pool |    1631 MB |    1646 MB |    1881 MB |  255580 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      61 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1634 MB |    1649 MB |    1884 MB |  255641 KB |
|       from large pool |    1631 MB |    1646 MB |    1881 MB |  255580 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      61 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1652 MB |    1672 MB |    1712 MB |   61440 KB |
|       from large pool |    1648 MB |    1668 MB |    1708 MB |   61440 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   17663 KB |   38563 KB |  545767 KB |  528104 KB |
|       from large pool |   16656 KB |   37555 KB |  542644 KB |  525988 KB |
|       from small pool |    1007 KB |    2038 KB |    3123 KB |    2116 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     307    |      85    |
|       from large pool |      83    |      84    |     126    |      43    |
|       from small pool |     139    |     139    |     181    |      42    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     307    |      85    |
|       from large pool |      83    |      84    |     126    |      43    |
|       from small pool |     139    |     139    |     181    |      42    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      16    |      17    |      19    |       3    |
|       from large pool |      14    |      15    |      17    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |       9    |      59    |      53    |
|       from large pool |       5    |       7    |      43    |      38    |
|       from small pool |       1    |       2    |      16    |      15    |
|===========================================================================|

[2021-12-22 12:14:39,424][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:39,424][fairseq.trainer][WARNING] - attempting to recover from OOM in forward/backward pass
[2021-12-22 12:14:39,426][fairseq_cli.train][INFO] - begin validation on "valid" subset
[2021-12-22 12:14:40,573][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 93.25 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:40,574][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1901 MB |  273102 KB |
|       from large pool |    1632 MB |    1646 MB |    1898 MB |  273037 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      65 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1901 MB |  273102 KB |
|       from large pool |    1632 MB |    1646 MB |    1898 MB |  273037 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      65 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1732 MB |   61440 KB |
|       from large pool |    1668 MB |    1668 MB |    1728 MB |   61440 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37846 KB |   38563 KB |  578636 KB |  540790 KB |
|       from large pool |   36839 KB |   37555 KB |  575508 KB |  538669 KB |
|       from small pool |    1007 KB |    2038 KB |    3127 KB |    2120 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     313    |      91    |
|       from large pool |      83    |      84    |     129    |      46    |
|       from small pool |     139    |     139    |     184    |      45    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     313    |      91    |
|       from large pool |      83    |      84    |     129    |      46    |
|       from small pool |     139    |     139    |     184    |      45    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      20    |       3    |
|       from large pool |      15    |      15    |      18    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      63    |      55    |
|       from large pool |       7    |       7    |      46    |      39    |
|       from small pool |       1    |       2    |      17    |      16    |
|===========================================================================|

[2021-12-22 12:14:40,575][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

[2021-12-22 12:14:40,575][fairseq.trainer][WARNING] - ran out of memory in validation step, retrying batch
[2021-12-22 12:14:40,595][fairseq.trainer][WARNING] - OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 73.25 MiB free; 1.63 GiB reserved in total by PyTorch)
[2021-12-22 12:14:40,596][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    1635 MB |    1649 MB |    1901 MB |  273102 KB |
|       from large pool |    1632 MB |    1646 MB |    1898 MB |  273037 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      65 KB |
|---------------------------------------------------------------------------|
| Active memory         |    1635 MB |    1649 MB |    1901 MB |  273102 KB |
|       from large pool |    1632 MB |    1646 MB |    1898 MB |  273037 KB |
|       from small pool |       3 MB |       3 MB |       3 MB |      65 KB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1672 MB |    1672 MB |    1732 MB |   61440 KB |
|       from large pool |    1668 MB |    1668 MB |    1728 MB |   61440 KB |
|       from small pool |       4 MB |       4 MB |       4 MB |       0 KB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   37846 KB |   38563 KB |  578636 KB |  540790 KB |
|       from large pool |   36839 KB |   37555 KB |  575508 KB |  538669 KB |
|       from small pool |    1007 KB |    2038 KB |    3127 KB |    2120 KB |
|---------------------------------------------------------------------------|
| Allocations           |     222    |     223    |     313    |      91    |
|       from large pool |      83    |      84    |     129    |      46    |
|       from small pool |     139    |     139    |     184    |      45    |
|---------------------------------------------------------------------------|
| Active allocs         |     222    |     223    |     313    |      91    |
|       from large pool |      83    |      84    |     129    |      46    |
|       from small pool |     139    |     139    |     184    |      45    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      17    |      17    |      20    |       3    |
|       from large pool |      15    |      15    |      18    |       3    |
|       from small pool |       2    |       2    |       2    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       8    |       9    |      63    |      55    |
|       from large pool |       7    |       7    |      46    |      39    |
|       from small pool |       1    |       2    |      17    |      16    |
|===========================================================================|

[2021-12-22 12:14:40,597][fairseq.trainer][WARNING] - |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

Traceback (most recent call last):
  File "/opt/wav2vec/fairseq/fairseq/trainer.py", line 910, in valid_step
    sample, self.model, self.criterion
  File "/opt/wav2vec/fairseq/fairseq/tasks/audio_pretraining.py", line 267, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/opt/wav2vec/fairseq/fairseq/tasks/fairseq_task.py", line 485, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/criterions/ctc.py", line 106, in forward
    net_output = model(**sample["net_input"])
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 190, in forward
    x = self.w2v_encoder(**kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 372, in forward
    x, padding_mask = self.w2v_model.extract_features(**w2v_args)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 631, in extract_features
    res = self.forward(source, padding_mask, mask=mask, features_only=True)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 486, in forward
    features = self.feature_extractor(source)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 741, in forward
    x = conv(x)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 260, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 93.25 MiB free; 1.63 GiB reserved in total by PyTorch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/wav2vec/fairseq/fairseq_cli/hydra_train.py", line 45, in hydra_main
    distributed_utils.call_main(cfg, pre_main)
  File "/opt/wav2vec/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq_cli/train.py", line 173, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/opt/wav2vec/fairseq/fairseq_cli/train.py", line 299, in train
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File "/opt/wav2vec/fairseq/fairseq_cli/train.py", line 385, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "/opt/wav2vec/fairseq/fairseq_cli/train.py", line 455, in validate
    trainer.valid_step(sample)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/opt/wav2vec/fairseq/fairseq/trainer.py", line 924, in valid_step
    return self.valid_step(sample, raise_oom=True)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/opt/wav2vec/fairseq/fairseq/trainer.py", line 925, in valid_step
    raise e
  File "/opt/wav2vec/fairseq/fairseq/trainer.py", line 910, in valid_step
    sample, self.model, self.criterion
  File "/opt/wav2vec/fairseq/fairseq/tasks/audio_pretraining.py", line 267, in valid_step
    loss, sample_size, logging_output = super().valid_step(sample, model, criterion)
  File "/opt/wav2vec/fairseq/fairseq/tasks/fairseq_task.py", line 485, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/criterions/ctc.py", line 106, in forward
    net_output = model(**sample["net_input"])
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 190, in forward
    x = self.w2v_encoder(**kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2_asr.py", line 372, in forward
    x, padding_mask = self.w2v_model.extract_features(**w2v_args)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 631, in extract_features
    res = self.forward(source, padding_mask, mask=mask, features_only=True)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 486, in forward
    features = self.feature_extractor(source)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/wav2vec/fairseq/fairseq/models/wav2vec/wav2vec2.py", line 741, in forward
    x = conv(x)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/container.py", line 119, in forward
    input = module(input)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/orchids/miniconda3/envs/fairseq/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 260, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 508.00 MiB (GPU 0; 10.92 GiB total capacity; 1.60 GiB already allocated; 73.25 MiB free; 1.63 GiB reserved in total by PyTorch)

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish, PID 31198... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.13MB uploaded (0.00MB deduped)wandb: | 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: / 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: - 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: \ 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: | 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: / 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: - 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: \ 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: | 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb: / 0.13MB of 0.13MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced finetuning: https://wandb.ai/vaishshells/ieee_worksop/runs/4dt82hf2
wandb: Find logs at: ./outputs/2021-12-22/12-12-25/wandb/run-20211222_121302-4dt82hf2/logs/debug.log
wandb: 

